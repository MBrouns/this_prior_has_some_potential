---
title: Priors with Great Potential
description: |
  Geez. Another way of interpreting Bayes Rule.
author:
  - name: Matthijs Brouns
  - name: Vincent Warmerdam
    url: koaning.io
date: 01-06-2020
output:
  distill::distill_article:
    self_contained: false
preview: examples.png
---

The goal of this document is to summarise a lesson we've had in the last year. 
We've done a lot of work on algorithmic bias (and [open-sourced it](https://scikit-lego.readthedocs.io/en/latest/index.html)) and
the main lessons we learned is that constraints are an amazing idea that deserve 
to be used more often in machine learning. This point drove us write the following 
formula on a whiteboard;

$$ \text{model} = \text{data} \times \text{constraints} $$

After writing it down, we noticed that we've seen this before but in a different notation.

$$ p(\theta | D) = \text{D | \theta} p(\theta) $$

It's poetic: maybe ... just maybe ... priors can be interpreted as constraints that we wish 
to impose on models. It is knowledge that we have about how the model *should* work even if
the data wants to push us in another direction. 

So what we'd like to do in this blogpost is explore the idea of constraints a bit more. First
by showcasing how our open source package deals with it but then showing how a probibalistic 
approach might be able to use bayes rule to go an extra mile. 

<aside>This document will contain the summaries, you can find the bigger project [here](https://github.com/MBrouns/this_prior_has_some_potential).</aside>

## Dataset and Fairness 

The dataset we'll be using can be found in [scikit-lego](https://scikit-lego.readthedocs.io/en/latest/index.html). 
It contains traffic arrests in Toronto and it is our job to predict if somebody is released after they
are arrested. It has attributes for skin color, gender, age, employment, citizenship, past interactions and date. 
We consider date, employment and citizenship to be proxies that go into the model while we keep gender,
skin color and age seperate as sensitive attributes that we want to remain fair on. 

The dataset is interesting because not only is there a fairness risk; there is also a balancing issue. 
The balancing issue can be dealt with by adding a `class_weight` parameter while the fairness 
can be dealt with in many ways ([exibit A](https://www.youtube.com/watch?v=cIGX-34GwJM), 
[exibit B](https://www.youtube.com/watch?v=Z8MEFI7ZJlA)). A favorable method (we think so) is to apply 
a hard constraint. Our [implementation](https://scikit-lego.readthedocs.io/en/latest/fairness.html#Equal-opportunity) 
of `EqualOpportunityClassifier` does this running a logistic regression constrained by the 
distance to the decision boundary in two groups.

```python
from sklearn.linear_model import LogisticRegression
from sklego.linear_model import EqualOpportunityClassifier

unfair_model = LogisticRegression(class_weight='balanced')
fair_model = EqualOpportunityClassifier(
    covariance_threshold=0.9, # strictness of threshold
    positive_target='Yes',    # name of the preferable label
    sensitive_cols=[0, 1, 2]  # columns in X that are considered sensitive
)

unfair_model.fit(X, y)
fair_model.fit(X, y)
```

The table below shows the cross-validated summary of the performance of both models.

There are a few things to note at this stage; 

- Accuracy and fairness don't go hand in hand. 
- The fair model is arguable still useful despite having a lower accuracy. 
- Having a fairness constraint based on distance to the decision boundary is one 
description of fairness. But, there may very well be others. In general though 
it is hard to guarantee a constraint unless the resulting optimisation problem is convex. 
- The current modelling approach allows us to make predictions but these predictions 
do not have uncertainty bounds. 
- Linear models are fine, but sometimes we may want to apply constraints to 
hierarchical models. 

## Probibalistic Programming
